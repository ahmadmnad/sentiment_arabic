{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "arabic sentiment analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSx3SNqra3aq",
        "outputId": "30ce4e2c-8f08-4fc6-d70e-6696c4c68bb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk \n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "import pandas as pd\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyArabic\n",
        "!pip install Tashaphyne"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abjjQtPxbwvf",
        "outputId": "f635abc4-73a3-4d3c-a11d-96d8a4d34965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyArabic\n",
            "  Downloading PyArabic-0.6.14-py3-none-any.whl (126 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 19.2 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20 kB 25.8 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 19.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61 kB 22.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 81 kB 23.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92 kB 25.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102 kB 23.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 112 kB 23.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122 kB 23.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 126 kB 23.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from PyArabic) (1.15.0)\n",
            "Installing collected packages: PyArabic\n",
            "Successfully installed PyArabic-0.6.14\n",
            "Collecting Tashaphyne\n",
            "  Downloading Tashaphyne-0.3.6-py3-none-any.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 27.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarabic in /usr/local/lib/python3.7/dist-packages (from Tashaphyne) (0.6.14)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from pyarabic->Tashaphyne) (1.15.0)\n",
            "Installing collected packages: Tashaphyne\n",
            "Successfully installed Tashaphyne-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import pyarabic.arabrepr\n",
        "from tashaphyne.stemming import ArabicLightStemmer\n",
        "from nltk import word_tokenize\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "vVxgHHAkb03I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arepr = pyarabic.arabrepr.ArabicRepr()\n",
        "repr = arepr.repr"
      ],
      "metadata": {
        "id": "mLY2212Eb3ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_ =['من',\n",
        " 'في',\n",
        " 'على',\n",
        " 'و',\n",
        " 'فى',\n",
        " 'يا',\n",
        " 'عن',\n",
        " 'مع',\n",
        " 'ان',\n",
        " 'هو',\n",
        " 'علي',\n",
        " 'ما',\n",
        " 'اللي',\n",
        " 'كل',\n",
        " 'بعد',\n",
        " 'ده',\n",
        " 'اليوم',\n",
        " 'أن',\n",
        " 'يوم',\n",
        " 'انا',\n",
        " 'إلى',\n",
        " 'كان',\n",
        " 'ايه',\n",
        " 'اللى',\n",
        " 'الى',\n",
        " 'دي',\n",
        " 'بين',\n",
        " 'انت',\n",
        " 'أنا',\n",
        " 'حتى',\n",
        " 'لما',\n",
        " 'فيه',\n",
        " 'هذا',\n",
        " 'واحد',\n",
        " 'احنا',\n",
        " 'اي',\n",
        " 'كده',\n",
        " 'إن',\n",
        " 'او',\n",
        " 'أو',\n",
        " 'عليه',\n",
        " 'ف',\n",
        " 'دى',\n",
        " 'مين',\n",
        " 'الي',\n",
        " 'كانت',\n",
        " 'أمام',\n",
        " 'زي',\n",
        " 'يكون',\n",
        " 'خلال',\n",
        " 'ع',\n",
        " 'كنت',\n",
        " 'هي',\n",
        " 'فيها',\n",
        " 'عند',\n",
        " 'التي',\n",
        " 'الذي',\n",
        " 'قال',\n",
        " 'هذه',\n",
        " 'قد',\n",
        " 'انه',\n",
        " 'ريتويت',\n",
        " 'بعض',\n",
        " 'أول',\n",
        " 'ايه',\n",
        " 'الان',\n",
        " 'أي',\n",
        " 'منذ',\n",
        " 'عليها',\n",
        " 'له',\n",
        " 'ال',\n",
        " 'تم',\n",
        " 'ب',\n",
        " 'دة',\n",
        " 'عليك',\n",
        " 'اى',\n",
        " 'كلها',\n",
        " 'اللتى',\n",
        " 'هى',\n",
        " 'دا',\n",
        " 'انك',\n",
        " 'وهو',\n",
        " 'ومن',\n",
        " 'منك',\n",
        " 'نحن',\n",
        " 'زى',\n",
        " 'أنت',\n",
        " 'انهم',\n",
        " 'معانا',\n",
        " 'حتي',\n",
        " 'وانا',\n",
        " 'عنه',\n",
        " 'إلي',\n",
        " 'ونحن',\n",
        " 'وانت',\n",
        " 'منكم',\n",
        " 'وان',\n",
        " 'معاهم',\n",
        " 'معايا',\n",
        " 'وأنا',\n",
        " 'عنها',\n",
        " 'إنه',\n",
        " 'اني',\n",
        " 'معك',\n",
        " 'اننا',\n",
        " 'فيهم',\n",
        " 'د',\n",
        " 'انتا',\n",
        " 'عنك',\n",
        " 'وهى',\n",
        " 'معا',\n",
        " 'آن',\n",
        " 'انتي',\n",
        " 'وأنت',\n",
        " 'وإن',\n",
        " 'ومع',\n",
        " 'وعن',\n",
        " 'معاكم',\n",
        " 'معاكو',\n",
        " 'معاها',\n",
        " 'وعليه',\n",
        " 'وانتم',\n",
        " 'وانتي',\n",
        " '¿',\n",
        " '|']"
      ],
      "metadata": {
        "id": "kt-7t92Cb6Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(text):\n",
        "    filtered_text = [] \n",
        "    for sentence in text:\n",
        "        sentence = re.sub(\"[إأآا]\", \"ا\", sentence)\n",
        "        sentence = re.sub(\"ى\", \"ي\", sentence)\n",
        "        sentence = re.sub(\"ؤ\", \"ء\", sentence)\n",
        "        sentence = re.sub(\"ئ\", \"ء\", sentence)\n",
        "        sentence = re.sub(\"ة\", \"ه\", sentence)\n",
        "        sentence = re.sub(\"گ\", \"ك\", sentence)\n",
        "        filtered_text.append(sentence)\n",
        "    return pd.DataFrame(data = filtered_text, columns = ['review'])\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    remove_stop_word = lambda x: ' '.join([word for word in x.split() if word not in stop_words_])\n",
        "    return text.apply(remove_stop_word)\n",
        "\n",
        "def remove_puncs(text):\n",
        "    punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation\n",
        "    filtered_text = [] \n",
        "    for sentence in text: \n",
        "        translator = str.maketrans('', '', punctuations)\n",
        "        filtered_text.append(sentence.translate(translator))\n",
        "    return filtered_text\n",
        "\n",
        "def remove_emojis(text):\n",
        "    remove_Emoji = lambda x : re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"\n",
        "        u\"\\U0001F300-\\U0001F5FF\"\n",
        "        u\"\\U0001F680-\\U0001F6FF\"\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "                           \"]+\", flags = re.UNICODE).sub(r'',x)\n",
        "    return text.apply(remove_Emoji)\n",
        "\n",
        "def stemming(text):\n",
        "    ArListem = ArabicLightStemmer()\n",
        "    each_lemma_word = []\n",
        "    each_lemma_sentence = []\n",
        "    for sentence in text:\n",
        "        for word in sentence.split():\n",
        "            stem = ArListem.light_stem(word)\n",
        "            each_lemma_word.append(stem)\n",
        "        each_lemma_sentence.append(\" \".join(each_lemma_word))\n",
        "        each_lemma_word.clear()\n",
        "    return each_lemma_sentence"
      ],
      "metadata": {
        "id": "NBPOXYOxb8xS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    s1 = normalize(text)\n",
        "    print('normalized!')\n",
        "    \n",
        "    s2 = remove_stopwords(s1['review'])\n",
        "    print('stop words removed!')\n",
        "    \n",
        "    s3 = remove_emojis(s2)\n",
        "    print('emojis removed! ')\n",
        "    \n",
        "    s4 =  remove_puncs(s3)\n",
        "    print('punctuations removed!')\n",
        "        \n",
        "    s5 = stemming(s4)\n",
        "    print('stemming done!')\n",
        "    \n",
        "    return s5"
      ],
      "metadata": {
        "id": "i5-_ztjnb_N4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "text='الفندق فيهم ممتاز .  الخدم  انتا سيئة  '\n",
        "print(text)\n",
        "token_text =sent_tokenize(text)\n",
        "print(token_text)\n",
        "ctt=clean_text(token_text)\n",
        "print(ctt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMqUjqvvcBzz",
        "outputId": "60567aca-1e62-4e7c-bdb5-57e0a845d307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "الفندق فيهم ممتاز .  الخدم  انتا سيئة  \n",
            "['الفندق فيهم ممتاز .', 'الخدم  انتا سيئة']\n",
            "normalized!\n",
            "stop words removed!\n",
            "emojis removed! \n",
            "punctuations removed!\n",
            "stemming done!\n",
            "['فندق ممتاز', 'خدم ءه']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_OZ921rVcMHf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}